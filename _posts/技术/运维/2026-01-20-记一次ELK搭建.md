---
layout: default1
title: "第一次ELK搭建"
date: 2025-12-10
categories: ['运维']
tags: ['ELK', '数据分析']
author: LiGen
---
# 1.引入

E（Elastic Search）L（Logstash）K（Kibana）是一套实时日志采集、可视化分析系统，因为我们的项目不大，日志不多，将Logstash替换为轻量级Filebeat进行日志采集

![](/assets/img/EL(F)K.png)

我们这次系统的搭建完全基于Docker进行，所有镜像使用官方`8.11.3`版本

# 2.搭建

因为我们使用Docker进行搭建，那么最好是创建一个网络，当然这个不是必要的，暂时是为了好管理，并且Docker容器在网络内部，可以直接通过容器名称进行访问
```bash
docker network create elk
```

# 2.1 Elastic Search
整个搭建流程先从ES开始，ES的官方镜像中，配置文件、数据存储位置、插件等都在`/usr/share/elasticsearch`路径下
如果没有配置文件，可以先默认启动一个容器，使用如下命令将需要的文件复制出来，再使用复制好的文件挂载
```bash
docker cp container:/usr/share/elasticsearch .
```
ES就使用默认启动了，以下是启动命令
```bash
docker run --name es --net elk -p 9200:9200  -p 9300:9300 \
       -e "discovery.type=single-node" \
       -e ES_JAVA_OPTS="-Xms512m -Xmx2048m"  \
       -v /volume5/docker/program/prod/elk/es/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml \
       -v /volume5/docker/program/prod/elk/es/data:/usr/share/elasticsearch/data \
       -v /volume5/docker/program/prod/elk/es/plugins:/usr/share/elasticsearch/plugins  \
       -d elasticsearch:8.11.3
```
启动后通过访问`url:9200`验证是否启动成功
ES默认用户名是`elastic`，进入容器修改密码
```bash
/bin/elasticsearch-reset-password -u elastic -i # elastic默认用户名，设置密码就好
```
输入用户名密码即可访问成功
```json
{
  "name" : "ef0907ddd494",
  "cluster_name" : "docker-cluster",
  "cluster_uuid" : "l3HAZFXMTbaQMAKNOkK5Og",
  "version" : {
    "number" : "8.11.3",
    "build_flavor" : "default",
    "build_type" : "docker",
    "build_hash" : "64cf052f3b56b1fd4449f5454cb88aca7e739d9a",
    "build_date" : "2023-12-08T11:33:53.634979452Z",
    "build_snapshot" : false,
    "lucene_version" : "9.8.0",
    "minimum_wire_compatibility_version" : "7.17.0",
    "minimum_index_compatibility_version" : "7.0.0"
  },
  "tagline" : "You Know, for Search"
}
```

# 2.2 Kibana
接下来安装Kibana，同样是默认启动，然后复制配置文件等
```bash
docker run --name kibana --net elk\
  -p 5601:5601 \
  -v /volume5/docker/program/prod/elk/kibana/config/kibana.yml:/usr/share/kibana/config/kibana.yml \
  -v /volume5/docker/program/prod/elk/kibana/data/:/usr/share/kibana/data \
  -d kibana:8.11.3
```
kibana的原始配置文件是这样
```yml
# Default Kibana configuration for docker target
server.host: "0.0.0.0"
server.shutdownTimeout: "5s"
elasticsearch.hosts: [ "http://elasticsearch:9200" ]
monitoring.ui.container.elasticsearch.enabled: true
```
我们要修改连接ES的地址，然后设置用户名密码
kibana不允许使用`elastic`这个用户，es中内置了`kibana_system`
这个用户只用户初始化kibana，在`http://es:5601`中可以使用`elastic`登录
所以添加
```yml
elasticsearch.username: kibana_system
elasticsearch.password: "123456"
```
# 2.3 filebeat

filebeat的原始配置文件
```yml
filebeat.config:
  modules:
    path: ${path.config}/modules.d/*.yml
    reload.enabled: false

processors:
  - add_cloud_metadata: ~
  - add_docker_metadata: ~

output.elasticsearch:
  hosts: '${ELASTICSEARCH_HOSTS:elasticsearch:9200}'
  username: '${ELASTICSEARCH_USERNAME:}'
  password: '${ELASTICSEARCH_PASSWORD:}'

```
filebeat配置挺重要的，它决定了当你没有logstash时，你的日志最终的收集方式

启动命令如下，`data`中存储了filebeat收集每个日志的偏移
```bash
docker run --name filebeat --net elk\
  -v /volume5/docker/program/prod/elk/filebeat/business/:/usr/share/filebeat/business/ \
  -v /volume5/docker/program/prod/elk/filebeat/data/:/usr/share/filebeat/data/ \
  -v /volume5/docker/program/prod/elk/filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml \
  -d elastic/filebeat:8.11.3
```
filebeat收集日志可以通过`grok`进行匹配，[grok在线调试](https://www.5axxw.com/tools/v2/grok.html)
因为不断需要不断调试，删除`data/registry/filebeat/log.json`或者重置里面的`offset`即可重新收集

1. 尝试grok
以下是符合我的日志`grok`
```grok
^%{TIMESTAMP_ISO8601:timestamp} \\| %{LOGLEVEL:log_level} %{INT:pid} \\| %{NOTSPACE:thread} \\[TID: %{DATA:tid}\\] %{JAVACLASS:methodname} \\| %{GREEDYDATA:message}
```
`filebeat`不能直接使用`grok`，需要将日志发送到ES后，在ES中使用`pipeline`处理，如下配置
```json
PUT _ingest/pipeline/schola-log-pipeline
{
  "description": "Parse Schola JSON logs",
  "processors": [
    {
      "grok": {
        "field": "message",
        "patterns": [
          "^%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:log_level} %{INT:pid} %{NOTSPACE:thread} \[TID: %{DATA:tid}\] %{JAVACLASS:methodname} %{GREEDYDATA:message}"
        ]
      }
    },
    {
      "rename": {
        "field": "parsed.log.level",
        "target_field": "level",
        "ignore_missing": true
      }
    },
    {
      "rename": {
        "field": "parsed.message",
        "target_field": "log_message",
        "ignore_missing": true
      }
    },
    {
      "rename": {
        "field": "parsed.@timestamp",
        "target_field": "timestamp",
        "ignore_missing": true
      }
    },
    {
      "remove": {
        "field": "parsed",
        "ignore_missing": true
      }
    }
  ]
}
```
创建好`pipeline`，可以使用如下命令做测试，如果能通过那基本没有问题，因为在`error`日志中会有堆栈，我们需要配置多行日志`multiline`
```
POST _ingest/pipeline/extract-schola-pipeline/_simulate
{
  "docs": [
    {
      "_source": {
        "message": "content"
      }
    }
  ]
}
```
配置如下，所有文件放在了同一个index中
```yml
filebeat.config:
  modules:
    path: ${path.config}/modules.d/*.yml
    reload.enabled: false

processors:
  - add_cloud_metadata: ~
  - add_docker_metadata: ~
  # 去除ANSI转义序列
  - script:
      lang: javascript
      id: remove_ansi
      source: |
        function process(event) {
          var message = event.Get("message");
          if (message !== undefined) {
            message = message.replace(/\u001b(\[39m|\[0;39m|\[1;33m|\[1;32m|\[1;31m|\[31m|\[34m)\.?/g, "").replace(/ *\| */g, " ");
            event.Put("message", message);
          }
        }

# 文件输入配置
filebeat.inputs:
  - type: log
    encoding: utf-8
    enabled: true
    ignore_older: "1440h"
    scan_frequency: "10s"
    paths:
      - /usr/share/filebeat/business/schola/schola-server-info*.log
    parsers:
    - multiline:
        type: pattern
        pattern: '^\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2}\.\d{3}'  # 以时间戳开头的行是新事件
        negate: true
        match: after
        max_lines: 200  # 防止堆栈跟踪过长
    fields:
      environment: schola_prod
      version: 4.1.0
      log_type: info
    # fields_under_root: true  # 重要：让字段出现在根级别
  - type: log
    encoding: utf-8
    enabled: true
    ignore_older: "1440h"
    scan_frequency: "10s"
    paths:
      - /usr/share/filebeat/business/schola/schola-server-warn*.log
    multiline:
        type: pattern
        pattern: '^\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2}\.\d{3}'  # 以时间戳开头的行是新事件
        negate: true
        match: after
        max_lines: 200  # 防止堆栈跟踪过长
    fields:
      environment: schola_prod
      version: 4.1.0
      log_type: warn
    # fields_under_root: true  # 重要：让字段出现在根级别
  - type: log
    encoding: utf-8
    enabled: true
    ignore_older: "1440h"
    scan_frequency: "10s"
    paths:
      - /usr/share/filebeat/business/schola/schola-server-error*.log
    parsers:
    - multiline:
        type: pattern
        pattern: '^\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2}\.\d{3}'  # 以时间戳开头的行是新事件
        negate: true
        match: after
        max_lines: 200  # 防止堆栈跟踪过长
    fields:
      environment: schola_prod
      version: 4.1.0
      log_type: error
    # fields_under_root: true  # 重要：让字段出现在根级别

# 输出配置
output.elasticsearch:
  hosts: ["http://es:9200"]
  username: elastic
  password: "123456"
  pipeline: extract-schola-pipeline
  # 根据日志级别指定不同的索引
  indices:
    - index: schola-info
      when.equals:
        fields.log_type: info
    - index: schola-warn
      when.equals:
        fields.log_type: warn
    - index: schola-error
      when.equals:
        fields.log_type: error

logging.level: debug
logging.selectors: ["elasticsearch"]
```

> 如果提示`{"log.level":"warn","@timestamp":"2026-01-23T13:51:49.859Z","log.logger":"elasticsearch","log.origin":{"file.name":"elasticsearch/client.go","file.line":449},"message":"Cannot index event (status=400): dropping event! Enable debug logs to view the event and cause.","service.name":"filebeat","ecs.version":"1.6.0"}`，说明ES pipeline配置有误，通过调试pipeline解决该问题
> 经过排查，因为日志使用`|`进行分隔，即使grok中将`|`进行了转义，还是导致报错，应该是转义还有问题，于是在`filebeat`中将`|`替换


# 3.使用

在上面使用`filebeat`将日志收集到了ES，我们可以在ES中使用`pipeline`将日志进行标准化，方便`kibana`进行分析

1. 原始日志
在不使用任何处理时，收集到的日志会放在`message`字段中，其他字段为一些元数据
```
@timestamp Jan 23, 2026 @ 22:53:22.669 agent.ephemeral _id f31837da-b6e6-4bfe-ba4d-54718f5a0a8d agent.id 49699473-6fb9-4f91-9c2b-1a554dc7e4ac agent.name 6253c31f9b58 agent.type filebeat agent.version 8.11.3 ecs.version 8.0.0 fields.environment schola_prod fields.version 4.1.0 host.name 6253c31f9b58 input.type log log.file.path /usr/share/filebeat/business/schola/schola-server-info.2025-11-14.0.log log.offset 853,371 message 2025-11-14 22:47:58.181 ERROR 467056 http-nio-48080-exec-2 [TID: N/A] c.i.y.m.b.s.task.BpmTaskServiceImpl [processTaskAssigned][taskId(e92f0e6c-c168-11f0-92f7-00163e0449d7) 没有找到流程实例]_id_1VY65sBcAYQrbr2fSD4_indexschola-log_score -
```
`@timestamp`是收集日志的时间
`log.file.path`是文件路径
`input.type`是filebeat中定义的类型,同样还有`fields`下的一些字段
`message`为收集的内容

2. 在ES中创建`pipeline`对日志进行分割，并删除必须要的字段
第一步：对日志分割
第二步：将自定义时间戳替换为`@timestamp`
第三步：删除不必要的字段
最终`pipeline`
```json
[
  {
    "grok": {
      "field": "message",
      "patterns": [
        "^%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:log_level} %{INT:pid} %{NOTSPACE:thread} \\[TID: %{DATA:tid}\\]%{SPACE:x1}%{JAVACLASS:methodname} %{GREEDYDATA:message}"
      ]
    }
  },
  {
    "date": {
      "field": "timestamp",
      "formats": [
        "yyyy-MM-dd HH:mm:ss.SSS"
      ],
      "timezone": "Asia/Shanghai"
    }
  },
  {
    "set": {
      "field": "log.level",
      "value": "{{log_level}}"
    }
  },
  {
    "remove": {
      "field": [
        "timestamp",
        "log.file.path",
        "log.offset",
        "x1",
        "log_level",
        "fields.log_type"
      ],
      "ignore_missing": true
    }
  }
]
```


# 4.Docker-Compose
